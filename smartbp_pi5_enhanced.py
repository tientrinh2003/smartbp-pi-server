#!/usr/bin/env python3

# -*- coding: utf-8 -*-



"""

SmartBP Pi5 Enhanced Server (clean)

- FastAPI server for audio inference with YAMNet (TFLite)

- Optional background microphone capture via sounddevice

- Optional camera snapshot via OpenCV

- Auto-handle Flex delegate if model contains SELECT_TF_OPS

"""



import os

import io

import glob

import sys

import wave

import time

import base64

import logging

import threading

from pathlib import Path

from typing import List, Optional, Tuple, Dict



import numpy as np

from fastapi import FastAPI, UploadFile, File, Response, Body
from pydantic import BaseModel

from fastapi.responses import JSONResponse, HTMLResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware

from pydantic import BaseModel



# ---- Config ----

MODEL_DIR = Path("/home/tien/smartbp/models")

MODEL_TFLITE = MODEL_DIR / "yamnet_finetuned_builtin.tflite"   # √Ü¬∞u ti√É¬™n builtin-only

MODEL_TFLITE_FALLBACK = MODEL_DIR / "yamnet_finetuned.tflite"  # c√É¬≥ SELECT_TF_OPS (Flex)

AUDIO_RATE = 16000                      # YAMNet expects 16 kHz mono float32

AUDIO_BG_SECONDS = 2.0                  # window 2.0s (model expects 32000 samples)

AUDIO_BG_ENABLE = True                  # b√°¬∫¬≠t/t√°¬∫¬Øt background audio

CAMERA_INDEX = 0                        # /dev/video0



# ---- Logging ----

logging.basicConfig(

    level=logging.INFO,

    format="%(asctime)s.%(msecs)03d [%(levelname)s] %(message)s",

    datefmt="%H:%M:%S",

)

logger = logging.getLogger("smartbp")



# ---- Optional imports ----

try:

    import sounddevice as sd

    HAVE_SD = True

except Exception as e:

    logger.warning("sounddevice not available: %s", e)

    HAVE_SD = False



try:

    import cv2

    HAVE_CV2 = True

except Exception as e:

    logger.warning("OpenCV not available: %s", e)

    HAVE_CV2 = False





# ============================================================================

# TFLite Interpreter loader (tflite-runtime first, then TF + Flex delegate)

# ============================================================================



class InferenceBackend:

    def __init__(self):

        self.impl = None               # callable: run_inference(waveform) -> np.ndarray

        self.input_detail = None

        self.output_detail = None

        self.backend_name = "uninitialized"



    def _try_load_tflite_runtime(self, model_path: Path) -> bool:

        """Try to load with tflite-runtime.Interpreter (no Flex)."""

        try:

            from tflite_runtime.interpreter import Interpreter

            logger.info("Loading TFLite (tflite-runtime) from: %s", model_path)

            interp = Interpreter(model_path=str(model_path))

            interp.allocate_tensors()

            in_det = interp.get_input_details()[0]

            out_det = interp.get_output_details()[0]



            def run_inference(waveform: np.ndarray) -> np.ndarray:

                interp.set_tensor(in_det["index"], waveform.astype(np.float32))

                interp.invoke()

                return interp.get_tensor(out_det["index"])



            self.impl = run_inference

            self.input_detail = in_det

            self.output_detail = out_det

            self.backend_name = f"tflite-runtime:{model_path.name}"

            return True

        except Exception as e:

            logger.info("tflite-runtime load failed: %s", e)

            return False



    def _find_flex_delegate(self) -> Optional[str]:

        """Locate TensorFlow Flex delegate shared object."""

        try:

            import site

            site_pkgs = site.getsitepackages()[0]

            cands = glob.glob(os.path.join(site_pkgs, "tensorflow", "lite", "experimental", "*flex*.so"))

            return cands[0] if cands else None

        except Exception:

            return None



    def _try_load_tf_with_flex(self, model_path: Path) -> bool:

        """Try to load with TF's TFLite Interpreter + Flex delegate."""

        try:

            import tensorflow as tf

            from tensorflow.lite.python.interpreter import Interpreter, load_delegate



            flex_lib = self._find_flex_delegate()

            delegates = []

            if flex_lib:

                logger.info("Using Flex delegate: %s", flex_lib)

                delegates = [load_delegate(flex_lib)]

            else:

                logger.warning("Flex delegate not found √¢‚Ç¨‚Äù will try without it (may fail).")



            logger.info("Loading TFLite (TF interpreter) from: %s", model_path)

            interp = Interpreter(model_path=str(model_path), experimental_delegates=delegates)

            interp.allocate_tensors()

            in_det = interp.get_input_details()[0]

            out_det = interp.get_output_details()[0]



            def run_inference(waveform: np.ndarray) -> np.ndarray:

                interp.set_tensor(in_det["index"], waveform.astype(np.float32))

                interp.invoke()

                return interp.get_tensor(out_det["index"])



            self.impl = run_inference

            self.input_detail = in_det

            self.output_detail = out_det

            self.backend_name = f"tf-lite-flex:{model_path.name}"

            return True

        except Exception as e:

            logger.error("TF+Flex interpreter load failed: %s", e)

            return False



    def load(self) -> None:

        """Load model in priority order."""

        # 1) builtin-only preferred

        if MODEL_TFLITE.exists() and self._try_load_tflite_runtime(MODEL_TFLITE):

            return

        # 2) fallback model (may need Flex)

        if MODEL_TFLITE_FALLBACK.exists() and self._try_load_tflite_runtime(MODEL_TFLITE_FALLBACK):

            return

        # 3) try TF interpreter + Flex for each

        if MODEL_TFLITE.exists() and self._try_load_tf_with_flex(MODEL_TFLITE):

            return

        if MODEL_TFLITE_FALLBACK.exists() and self._try_load_tf_with_flex(MODEL_TFLITE_FALLBACK):

            return

        raise RuntimeError("No TFLite model could be loaded with available backends.")



    def run(self, waveform: np.ndarray) -> np.ndarray:

        if self.impl is None:

            raise RuntimeError("Inference backend not initialized")

        return self.impl(waveform)





backend = InferenceBackend()





# ============================================================================

# Audio utils: WAV decode, resample to 16k, normalization

# ============================================================================



def read_wav_pcm16_to_float32(data: bytes) -> Tuple[np.ndarray, int]:

    """

    Parse a WAV PCM16 mono/stereo into float32 [-1, 1], return (waveform, sample_rate).

    """

    with wave.open(io.BytesIO(data), "rb") as wf:

        n_channels = wf.getnchannels()

        sampwidth = wf.getsampwidth()

        framerate = wf.getframerate()

        n_frames = wf.getnframes()



        if sampwidth != 2:

            raise ValueError("Only PCM16 WAV is supported (sampwidth=2)")

        frames = wf.readframes(n_frames)

        audio = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32768.0

        if n_channels == 2:

            audio = audio.reshape(-1, 2).mean(axis=1)

        return audio, framerate





def simple_resample_1d(x: np.ndarray, src_rate: int, dst_rate: int) -> np.ndarray:

    """Linear resample (no dependency on librosa/scipy)."""

    if src_rate == dst_rate:

        return x

    duration = len(x) / float(src_rate)

    t_src = np.linspace(0.0, duration, num=len(x), endpoint=False)

    n_dst = int(round(duration * dst_rate))

    t_dst = np.linspace(0.0, duration, num=n_dst, endpoint=False)

    return np.interp(t_dst, t_src, x).astype(np.float32)





def pad_or_trim_to_seconds(x: np.ndarray, seconds: float, rate: int) -> np.ndarray:

    target = int(round(seconds * rate))

    if len(x) == target:

        return x

    if len(x) > target:

        return x[:target]

    # pad at end

    out = np.zeros((target,), dtype=np.float32)

    out[:len(x)] = x

    return out





def topk_indices(scores: np.ndarray, k: int = 5) -> List[Tuple[int, float]]:

    """Return top-k (index, prob) pairs from scores (1D)."""

    scores = scores.flatten()

    idx = np.argsort(scores)[::-1][:k]

    return [(int(i), float(scores[i])) for i in idx]





# ============================================================================

# Background audio thread (optional)

# ============================================================================



class AudioState:

    def __init__(self):

        self.lock = threading.Lock()

        self.buffer = np.array([], dtype=np.float32)

        self.last_scores: List[Tuple[int, float]] = []

        self.last_inference_at: float = 0.0  # timestamp of last successful inference

        self.running = False

        self.thread: Optional[threading.Thread] = None





audio_state = AudioState()

# ---- Camera State Management ----
class CameraState:
    def __init__(self):
        self.streaming_active = False
        self.lock = threading.Lock()
    
    def set_streaming(self, active: bool):
        with self.lock:
            self.streaming_active = active
            if not active:
                logger.info("üì∑ Camera streaming stopped")

camera_state = CameraState()

def sd_callback(indata, frames, time_info, status):

    if status:

        logger.warning("sounddevice status: %s", status)
    
    # Audio callback - logging disabled to reduce spam

    with audio_state.lock:

        audio_state.buffer = np.append(audio_state.buffer, indata[:, 0].astype(np.float32))





def audio_loop():

    logger.info("√∞≈∏≈Ω‚Ñ¢√Ø¬∏¬è  Background audio started (%.1fs window @ %d Hz).", AUDIO_BG_SECONDS, AUDIO_RATE)

    buf_needed = int(AUDIO_RATE * AUDIO_BG_SECONDS)

    stream = None

    try:

        # Auto-detect the first available microphone input device
        devices = sd.query_devices()
        input_device = None
        
        # Find first device with input channels (microphone)
        for i, device_info in enumerate(devices):
            if device_info['max_input_channels'] > 0:
                input_device = i
                logger.info(f"üé§ Using microphone: {device_info['name']} (device {i})")
                break
        
        if input_device is None:
            logger.warning("No input audio device found, using default")
            input_device = None  # Let sounddevice choose default
            
        stream = sd.InputStream(

            samplerate=AUDIO_RATE,

            channels=1,

            dtype="float32",

            callback=sd_callback,
            
            device=input_device,  # Auto-detected or default

        )

        stream.start()

        # Status reporting counter
        status_counter = 0
        
        while audio_state.running:

            wf = None
            status_counter += 1

            with audio_state.lock:
                current_buffer_size = len(audio_state.buffer)
                if current_buffer_size >= buf_needed:

                    wf = audio_state.buffer[:buf_needed].copy()

                    audio_state.buffer = audio_state.buffer[buf_needed:]
                    # Processing inference - logging disabled to reduce spam
                else:
                    # Show buffer status every 20 cycles (~1 second)
                    if status_counter % 20 == 0:
                        logger.info(f"üìä Buffer: {current_buffer_size}/{buf_needed} ({current_buffer_size/buf_needed*100:.0f}%) - waiting...")

            if wf is not None:

                # Infer with real audio data

                scores = backend.run(np.expand_dims(wf, axis=0))
                
                # Process results and update state
                results = topk_indices(scores, k=5)
                top_class, top_score = results[0]
                
                # Debug: show raw scores and shape
                raw_scores = scores[0]  # Get first batch
                logger.info(f"üîç Model output shape: {raw_scores.shape}, values: {raw_scores}")
                
                # Handle different output shapes
                if len(raw_scores) == 1:
                    # Binary classification with single output (sigmoid)
                    speech_prob = float(raw_scores[0])
                    speech_status = "SPEECH" if speech_prob > 0.5 else "NO_SPEECH"
                    confidence = f"{speech_prob:.3f}"
                elif len(raw_scores) >= 2:
                    # Multi-class with softmax
                    logger.info(f"üîç Raw scores - class 0 (no_speech): {raw_scores[0]:.6f}, class 1 (speech): {raw_scores[1]:.6f}")
                    speech_status = "SPEECH" if top_class == 1 else "NO_SPEECH"
                    confidence = f"{top_score:.3f}"
                else:
                    speech_status = "UNKNOWN"
                    confidence = "0.000"
                
                logger.info(f"üé§ Speech Detection: {speech_status} (confidence: {confidence})")

                with audio_state.lock:
                    if len(raw_scores) == 1:
                        # Single output (sigmoid) - convert to binary classification format
                        speech_prob = float(raw_scores[0])
                        if speech_prob > 0.5:
                            # Speech detected - class 1 with high confidence
                            audio_state.last_scores = [(1, speech_prob), (0, 1.0-speech_prob)]
                            logger.info(f"üîÑ Converted to: class 1 (speech) = {speech_prob:.3f}, class 0 = {1.0-speech_prob:.3f}")
                        else:
                            # No speech - class 0 with high confidence
                            audio_state.last_scores = [(0, 1.0-speech_prob), (1, speech_prob)]
                            logger.info(f"üîÑ Converted to: class 0 (no_speech) = {1.0-speech_prob:.3f}, class 1 = {speech_prob:.3f}")
                    else:
                        audio_state.last_scores = results
                    audio_state.last_inference_at = time.time()

            else:

                # Not enough buffered audio yet; keep previous inference scores.
                # Initialize baseline only if we have never inferred.
                with audio_state.lock:
                    if not audio_state.last_scores:
                        audio_state.last_scores = [(0, 0.99), (1, 0.01)]

            time.sleep(0.05)

    except Exception as e:

        logger.error("Audio loop error: %s", e)

    finally:

        if stream:

            stream.stop()

            stream.close()

        logger.info("√∞≈∏≈Ω‚Ñ¢√Ø¬∏¬è  Background audio stopped.")





def start_audio_if_possible():

    if not AUDIO_BG_ENABLE:

        logger.info("Background audio disabled by config.")

        return

    if not HAVE_SD:

        logger.warning("sounddevice not available; skip background audio.")

        return

    if audio_state.running:

        return

    audio_state.running = True

    audio_state.thread = threading.Thread(target=audio_loop, daemon=True)

    audio_state.thread.start()





def stop_audio():

    if not audio_state.running:

        return

    audio_state.running = False

    if audio_state.thread:

        audio_state.thread.join(timeout=2.0)

        audio_state.thread = None





# ============================================================================

# Camera snapshot

# ============================================================================



def capture_jpeg() -> Optional[bytes]:

    if not HAVE_CV2:

        return None

    cap = cv2.VideoCapture(CAMERA_INDEX)

    if not cap.isOpened():

        return None

    try:

        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)

        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

        ok, frame = cap.read()

        if not ok:

            return None

        ok, buf = cv2.imencode(".jpg", frame)

        if not ok:

            return None

        return buf.tobytes()

    finally:

        cap.release()





# ============================================================================

# FastAPI app

# ============================================================================



app = FastAPI(title="SmartBP Pi5 Enhanced", version="1.0.0")

# Add CORS middleware to allow requests from Next.js frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://192.168.22.65:3000"],  # Next.js dev server
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class InferResponse(BaseModel):

    backend: str

    topk: List[Tuple[int, float]]





@app.on_event("startup")

def on_startup():

    # Load interpreter

    backend.load()

    logger.info("Inference backend ready: %s", backend.backend_name)

    # Audio will be started only when monitoring is requested
    logger.info("üéØ Audio monitoring ready - waiting for start command")





@app.on_event("shutdown")

def on_shutdown():

    stop_audio()

    logger.info("Server shutdown")





@app.get("/", response_class=HTMLResponse)

def root_page():

    html = f"""

    <html>

      <head><title>SmartBP Pi5 Enhanced</title></head>

      <body style="font-family: sans-serif;">

        <h2>SmartBP Pi5 Enhanced</h2>

        <ul>

          <li>Backend: <b>{backend.backend_name}</b></li>

          <li><a href="/api/health">/api/health</a></li>

          <li><a href="/api/model">/api/model</a></li>

          <li><a href="/api/speech/status">/api/speech/status</a></li>

          <li>POST /api/speech/infer (multipart/form-data, file field: <code>file</code>, WAV PCM16)</li>

          <li><a href="/api/camera/snapshot">/api/camera/snapshot</a> (if OpenCV & camera available)</li>
          <li><a href="/api/camera/stream">/api/camera/stream</a> (MJPEG stream with AI overlay)</li>

        </ul>

        <p>Tip: Use <code>curl</code> to test:</p>

        <pre>

curl -F "file=@sample.wav" http://localhost:8000/api/speech/infer

        </pre>

      </body>

    </html>

    """

    return HTMLResponse(html)





@app.get("/api/health")

def api_health():

    return {"ok": True, "backend": backend.backend_name}





@app.get("/api/model")

def api_model():

    return {

        "backend": backend.backend_name,

        "input": backend.input_detail,

        "output": backend.output_detail,

        "expects_rate": AUDIO_RATE,

        "window_seconds": AUDIO_BG_SECONDS,

    }





@app.get("/api/speech/status", response_model=InferResponse)

def api_speech_status():

    with audio_state.lock:

        topk = list(audio_state.last_scores)

    return InferResponse(backend=backend.backend_name, topk=topk)


@app.get("/api/ai/status")
def api_ai_status():
    """Get current AI speech detection status"""
    
    with audio_state.lock:
        topk = list(audio_state.last_scores)
    
    # Check if we have valid data
    if not topk or len(topk) == 0:
        logger.info("üé§ API Status: no_data")
        logger.info("üì° API Response: is_speaking=False")
        return JSONResponse({
            "is_speaking": False,
            "confidence": 0.0,
            "status": "no_data",
            "timestamp": time.time()
        })
    
    # Binary model: topk = [(class_idx, probability)]
    top_class, top_prob = topk[0]
    
    # class 0 = no_speech, class 1 = speech
    is_speaking = (top_class == 1)
    confidence = top_prob
    
    status = "speaking" if is_speaking else "quiet"
    
    logger.info(f"üé§ API Status: {status} (confidence: {confidence:.3f}, class: {top_class})")
    logger.info(f"üì° API Response: is_speaking={is_speaking}")
    
    return JSONResponse({
        "is_speaking": is_speaking,
        "confidence": confidence,
        "status": status,
        "timestamp": time.time()
    })





@app.post("/api/speech/infer", response_model=InferResponse)

async def api_speech_infer(file: UploadFile = File(...)):

    data = await file.read()

    wav, rate = read_wav_pcm16_to_float32(data)

    wav = simple_resample_1d(wav, rate, AUDIO_RATE)

    wav = pad_or_trim_to_seconds(wav, seconds=2.0, rate=AUDIO_RATE)  # 2s for 32000 samples

    scores = backend.run(np.expand_dims(wav, axis=0))

    topk = topk_indices(scores, k=5)

    return InferResponse(backend=backend.backend_name, topk=topk)





@app.get("/api/camera/snapshot")

def api_camera_snapshot():

    img = capture_jpeg()

    if img is None:

        return JSONResponse({"error": "Camera not available"}, status_code=404)

    return Response(content=img, media_type="image/jpeg")


def get_current_ai_status():
    """Helper function to get current AI status for camera overlay"""
    try:
        with audio_state.lock:
            topk = list(audio_state.last_scores)
        
        if not topk or len(topk) == 0:
            return {
                "speech": {
                    "status": "no_data",
                    "confidence": 0.0,
                    "timestamp": time.time(),
                    "color": "green",
                    "is_speaking": False
                }
            }
        
        # Binary model: topk = [(class_idx, probability)]
        top_class, top_prob = topk[0]
        
        # class 0 = no_speech, class 1 = speech
        is_speaking = (top_class == 1)
        confidence = top_prob
        
        if is_speaking:
            if confidence > 0.8:
                status = "speaking"
                color = "red"
            else:
                status = "maybe_speaking"
                color = "yellow"
        else:
            status = "quiet"
            color = "green"
        
        return {
            "speech": {
                "status": status,
                "confidence": confidence,
                "timestamp": time.time(),
                "color": color,
                "is_speaking": is_speaking
            }
        }
    except Exception as e:
        logger.error(f"Error getting AI status: {e}")
        return None


@app.post("/api/monitoring/start")
def start_monitoring():
    """Start audio and camera monitoring"""
    global audio_state
    try:
        if not audio_state.running:
            # Start audio monitoring using existing function
            start_audio_if_possible()
            logger.info("üì¢ Monitoring started - audio and camera active")
        else:
            logger.info("üì¢ Monitoring already running")
        
        return {"success": True, "message": "Monitoring started", "services": ["audio", "camera"]}
    except Exception as e:
        logger.error(f"Failed to start monitoring: {e}")
        return {"success": False, "error": str(e)}


@app.post("/api/monitoring/stop") 
def stop_monitoring():
    """Stop audio monitoring and camera streaming"""
    global audio_state, camera_state
    try:
        # Stop audio using existing function
        stop_audio()
        # Signal camera to stop streaming
        camera_state.set_streaming(False)
        logger.info("üîá Monitoring stopped - audio and camera disabled")
        return {"success": True, "message": "Monitoring stopped", "services": ["audio", "camera"]}
    except Exception as e:
        logger.error(f"Failed to stop monitoring: {e}")
        return {"success": False, "error": str(e)}


@app.post("/api/bluetooth/scan")
async def scan_bluetooth():
    """Scan for nearby Bluetooth devices"""
    try:
        from bluetooth_scanner import scan_bluetooth_devices
        devices = await scan_bluetooth_devices(scan_duration=8)
        return {"success": True, "devices": devices, "count": len(devices)}
    except Exception as e:
        logger.error(f"Bluetooth scan error: {e}")
        return {"success": False, "error": str(e), "devices": []}


class MeasureRequest(BaseModel):
    device_address: str

@app.post("/api/bluetooth/measure")
async def measure_blood_pressure(request: MeasureRequest):
    """
    Measure blood pressure from Bluetooth device
    Body: { "device_address": "00:5F:BF:3A:51:BD" }
    """
    try:
        device_address = request.device_address
        
        if not device_address:
            return JSONResponse({
                "success": False, 
                "error": "Missing device_address"
            }, status_code=400)
        
        logger.info(f"ü©∫ B·∫Øt ƒë·∫ßu ƒëo huy·∫øt √°p t·ª´ thi·∫øt b·ªã: {device_address}")
        
        from bluetooth_bp_client import measure_once
        result = await measure_once(device_address, timeout=120)
        
        if result:
            logger.info(f"‚úÖ API tr·∫£ v·ªÅ k·∫øt qu·∫£: {result}")
            return JSONResponse({
                "success": True,
                "data": result,
                "message": "ƒêo huy·∫øt √°p th√†nh c√¥ng"
            })
        else:
            logger.warning("‚ö†Ô∏è Kh√¥ng nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ thi·∫øt b·ªã")
            return JSONResponse({
                "success": False, 
                "error": "Kh√¥ng nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ thi·∫øt b·ªã. Vui l√≤ng b·∫≠t ch·∫ø ƒë·ªô ƒëo tr√™n m√°y."
            }, status_code=408)
    
    except Exception as e:
        logger.error(f"Measurement error: {e}")
        return JSONResponse({
            "success": False, 
            "error": str(e)
        }, status_code=500)


@app.get("/api/camera/stream")
def api_camera_stream():
    """MJPEG streaming endpoint for camera with AI overlay"""
    if not HAVE_CV2:
        return JSONResponse({"error": "OpenCV not available for streaming"}, status_code=404)
    
    def generate_mjpeg():
        import time
        logger.info("üìπ Camera streaming started with AI overlay")
        
        cap = None
        try:
            cap = cv2.VideoCapture(CAMERA_INDEX)
            if not cap.isOpened():
                logger.error("Cannot open camera for streaming")
                return
            
            # Set camera properties for better performance
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            cap.set(cv2.CAP_PROP_FPS, 20)
            cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Reduce buffer to minimize lag
            
            frame_count = 0
            camera_state.set_streaming(True)
            last_ai_check = 0
            cached_ai_data = None
            
            while camera_state.streaming_active:
                ret, frame = cap.read()
                if not ret:
                    logger.warning("Failed to read frame from camera")
                    break
                
                # Get AI status only every 0.5s to reduce overhead
                current_time = time.time()
                if current_time - last_ai_check > 0.5:
                    try:
                        cached_ai_data = get_current_ai_status()
                        last_ai_check = current_time
                    except Exception as e:
                        logger.error(f"Error getting AI status: {e}")
                        cached_ai_data = None
                
                # Add AI overlay with cached data
                try:
                    if cached_ai_data and cached_ai_data.get('speech'):
                        speech = cached_ai_data['speech']
                        status = speech.get('status', 'unknown')
                        confidence = speech.get('confidence', 0.0)
                        is_speaking = speech.get('is_speaking', False)
                        
                        # Simpler overlay for better performance
                        if is_speaking:
                            text = f"Dang noi ({confidence:.0%})"
                            color = (0, 0, 255)  # Red
                        else:
                            text = f"Im lang ({confidence:.0%})"
                            color = (0, 255, 0)  # Green
                        
                        # Draw text with shadow for visibility
                        cv2.putText(frame, text, (12, 32), cv2.FONT_HERSHEY_SIMPLEX, 
                                   0.8, (0, 0, 0), 3)  # Shadow
                        cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                                   0.8, color, 2)  # Text
                        
                except Exception as e:
                    # Silently fail overlay to not interrupt stream
                    pass
                
                # Encode frame as JPEG with lower quality for speed
                ret, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 75])
                if not ret:
                    break
                    
                frame_bytes = buffer.tobytes()
                
                # Yield MJPEG frame
                yield (b'--frame\r\n'
                       b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
                
                frame_count += 1
                time.sleep(0.03)  # ~30 FPS max (reduced sleep for smoother video)
                
        except GeneratorExit:
            # Client disconnected - clean up immediately
            logger.info("üì∑ Client disconnected from stream")
        except Exception as e:
            logger.error(f"Camera streaming error: {e}")
        finally:
            if cap is not None and cap.isOpened():
                cap.release()
                logger.info("üì∑ Camera released")
            camera_state.set_streaming(False)
            logger.info("üìπ Camera streaming ended")
    
    return StreamingResponse(
        generate_mjpeg(),
        media_type="multipart/x-mixed-replace; boundary=frame"
    )


# ============================================================================

# Entrypoint (uvicorn)

# ============================================================================

def check_system_status():
    """Check and report system component status."""
    logger.info("üîç System Component Check:")
    
    # Check model file
    model_found = False
    if MODEL_TFLITE.exists():
        logger.info(f"‚úÖ Model file ready: {MODEL_TFLITE} ({MODEL_TFLITE.stat().st_size / 1024 / 1024:.1f}MB)")
        model_found = True
    elif MODEL_TFLITE_FALLBACK.exists():
        logger.info(f"‚úÖ Fallback model ready: {MODEL_TFLITE_FALLBACK} ({MODEL_TFLITE_FALLBACK.stat().st_size / 1024 / 1024:.1f}MB)")
        model_found = True
    else:
        logger.error(f"‚ùå No model files found: {MODEL_TFLITE} or {MODEL_TFLITE_FALLBACK}")
    
    # Check camera
    if HAVE_CV2:
        try:
            test_cap = cv2.VideoCapture(CAMERA_INDEX)
            if test_cap.isOpened():
                ret, frame = test_cap.read()
                if ret:
                    logger.info(f"‚úÖ Camera ready: /dev/video{CAMERA_INDEX} ({frame.shape[1]}x{frame.shape[0]})")
                else:
                    logger.error("‚ùå Camera capture test failed")
                test_cap.release()
            else:
                logger.error(f"‚ùå Camera not accessible: /dev/video{CAMERA_INDEX}")
        except Exception as e:
            logger.error(f"‚ùå Camera error: {e}")
    else:
        logger.error("‚ùå OpenCV not available")
    
    # Check microphone
    if HAVE_SD:
        try:
            devices = sd.query_devices()
            mic_devices = [i for i, d in enumerate(devices) if d['max_input_channels'] > 0]
            if mic_devices:
                logger.info(f"‚úÖ Microphone ready: {len(mic_devices)} input device(s) available")
                for i in mic_devices[:2]:  # Show first 2 devices
                    logger.info(f"   üì± Device {i}: {devices[i]['name']} ({int(devices[i]['default_samplerate'])}Hz)")
            else:
                logger.error("‚ùå No microphone devices found")
        except Exception as e:
            logger.error(f"‚ùå Microphone error: {e}")
    else:
        logger.error("‚ùå sounddevice not available")


if __name__ == "__main__":

    import uvicorn

    host = os.environ.get("HOST", "0.0.0.0")

    port = int(os.environ.get("PORT", "8000"))

    reload_flag = False  # √Ñ‚Äò√°¬ª∆í False tr√É¬™n Pi cho nh√°¬∫¬π

    logger.info("√∞≈∏≈°‚Ç¨ Starting server on %s:%d", host, port)

    uvicorn.run("smartbp_pi5_enhanced:app", host=host, port=port, reload=reload_flag)